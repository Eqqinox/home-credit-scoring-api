"""
Script pour calculer les mÃ©triques baseline depuis les donnÃ©es de production PostgreSQL.

Ce script extrait les statistiques dÃ©taillÃ©es (mean, median, percentiles, throughput)
depuis les 1,166 prÃ©dictions stockÃ©es dans PostgreSQL pour Ã©tablir une baseline
de performance avant optimisation.

Usage:
    python src/scripts/calculate_baseline_metrics.py

Output:
    reports/benchmarks/baseline_production.json
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

import psycopg2
from psycopg2.extras import RealDictCursor


def get_database_url() -> str:
    """Retourne l'URL de connexion PostgreSQL."""
    return "postgresql://moon:moon@localhost:5432/credit_scoring_prod"


def calculate_metrics_from_postgres() -> Dict[str, Any]:
    """
    Calcule les mÃ©triques de performance depuis PostgreSQL.

    Returns:
        Dictionnaire avec toutes les mÃ©triques (mean, median, percentiles, throughput).
    """
    db_url = get_database_url()

    # Parser l'URL PostgreSQL
    # Format: postgresql://user:password@host:port/database
    parts = db_url.replace("postgresql://", "").split("@")
    user_pass = parts[0].split(":")
    host_port_db = parts[1].split("/")
    host_port = host_port_db[0].split(":")

    conn_params = {
        "user": user_pass[0],
        "password": user_pass[1],
        "host": host_port[0],
        "port": host_port[1],
        "database": host_port_db[1]
    }

    print(f"ğŸ“Š Connexion Ã  PostgreSQL : {conn_params['database']}@{conn_params['host']}...")

    conn = psycopg2.connect(**conn_params)
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    # RequÃªte SQL pour calculer toutes les statistiques
    query = """
    SELECT
        COUNT(*) as n_samples,
        MIN(timestamp) as date_start,
        MAX(timestamp) as date_end,

        -- Total response time
        ROUND(AVG(total_response_time_ms)::numeric, 2) as total_mean,
        ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_response_time_ms)::numeric, 2) as total_median,
        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_response_time_ms)::numeric, 2) as total_p95,
        ROUND(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY total_response_time_ms)::numeric, 2) as total_p99,
        ROUND(MIN(total_response_time_ms)::numeric, 2) as total_min,
        ROUND(MAX(total_response_time_ms)::numeric, 2) as total_max,
        ROUND(STDDEV(total_response_time_ms)::numeric, 2) as total_std,

        -- Preprocessing time
        ROUND(AVG(preprocessing_time_ms)::numeric, 2) as prep_mean,
        ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY preprocessing_time_ms)::numeric, 2) as prep_median,
        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY preprocessing_time_ms)::numeric, 2) as prep_p95,
        ROUND(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY preprocessing_time_ms)::numeric, 2) as prep_p99,
        ROUND(MIN(preprocessing_time_ms)::numeric, 2) as prep_min,
        ROUND(MAX(preprocessing_time_ms)::numeric, 2) as prep_max,
        ROUND(STDDEV(preprocessing_time_ms)::numeric, 2) as prep_std,

        -- Inference time
        ROUND(AVG(inference_time_ms)::numeric, 2) as inf_mean,
        ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY inference_time_ms)::numeric, 2) as inf_median,
        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY inference_time_ms)::numeric, 2) as inf_p95,
        ROUND(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY inference_time_ms)::numeric, 2) as inf_p99,
        ROUND(MIN(inference_time_ms)::numeric, 2) as inf_min,
        ROUND(MAX(inference_time_ms)::numeric, 2) as inf_max,
        ROUND(STDDEV(inference_time_ms)::numeric, 2) as inf_std

    FROM predictions
    WHERE
        total_response_time_ms IS NOT NULL
        AND preprocessing_time_ms IS NOT NULL
        AND inference_time_ms IS NOT NULL;
    """

    print("ğŸ” ExÃ©cution de la requÃªte SQL...")
    cursor.execute(query)
    result = cursor.fetchone()

    cursor.close()
    conn.close()

    # Convertir le rÃ©sultat en dictionnaire Python standard
    result = dict(result)

    # Calculer les pourcentages et le throughput
    prep_percentage = round((float(result['prep_mean']) / float(result['total_mean'])) * 100, 1)
    inf_percentage = round((float(result['inf_mean']) / float(result['total_mean'])) * 100, 1)

    # Throughput = 1000ms / mean_total_ms pour avoir prÃ©dictions/seconde
    throughput_per_second = round(1000 / float(result['total_mean']), 2)
    throughput_per_minute = round(throughput_per_second * 60, 0)

    # Construire le JSON de sortie
    metrics = {
        "source": "production_data",
        "database": "credit_scoring_prod",
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "n_samples": result['n_samples'],
        "date_range": {
            "start": result['date_start'].strftime("%Y-%m-%d") if result['date_start'] else None,
            "end": result['date_end'].strftime("%Y-%m-%d") if result['date_end'] else None
        },

        "total_time_ms": {
            "mean": float(result['total_mean']),
            "median": float(result['total_median']),
            "p50": float(result['total_median']),
            "p95": float(result['total_p95']),
            "p99": float(result['total_p99']),
            "min": float(result['total_min']),
            "max": float(result['total_max']),
            "std": float(result['total_std'])
        },

        "preprocessing_time_ms": {
            "mean": float(result['prep_mean']),
            "median": float(result['prep_median']),
            "p50": float(result['prep_median']),
            "p95": float(result['prep_p95']),
            "p99": float(result['prep_p99']),
            "min": float(result['prep_min']),
            "max": float(result['prep_max']),
            "std": float(result['prep_std']),
            "percentage_of_total": prep_percentage
        },

        "inference_time_ms": {
            "mean": float(result['inf_mean']),
            "median": float(result['inf_median']),
            "p50": float(result['inf_median']),
            "p95": float(result['inf_p95']),
            "p99": float(result['inf_p99']),
            "min": float(result['inf_min']),
            "max": float(result['inf_max']),
            "std": float(result['inf_std']),
            "percentage_of_total": inf_percentage
        },

        "throughput": {
            "predictions_per_second": throughput_per_second,
            "predictions_per_minute": int(throughput_per_minute)
        }
    }

    return metrics


def save_metrics_to_json(metrics: Dict[str, Any], output_path: Path) -> None:
    """
    Sauvegarde les mÃ©triques dans un fichier JSON.

    Args:
        metrics: Dictionnaire des mÃ©triques.
        output_path: Chemin du fichier de sortie.
    """
    # CrÃ©er le rÃ©pertoire si nÃ©cessaire
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)

    print(f"âœ… MÃ©triques sauvegardÃ©es : {output_path}")


def print_summary(metrics: Dict[str, Any]) -> None:
    """Affiche un rÃ©sumÃ© des mÃ©triques calculÃ©es."""
    print("\n" + "="*70)
    print("ğŸ“Š MÃ‰TRIQUES BASELINE PRODUCTION")
    print("="*70)
    print(f"\nğŸ—„ï¸  Source : {metrics['source']}")
    print(f"ğŸ“… PÃ©riode : {metrics['date_range']['start']} â†’ {metrics['date_range']['end']}")
    print(f"ğŸ“ˆ Ã‰chantillons : {metrics['n_samples']:,} prÃ©dictions")

    print("\nâ±ï¸  TEMPS TOTAL")
    print(f"   Mean   : {metrics['total_time_ms']['mean']:.2f} ms")
    print(f"   Median : {metrics['total_time_ms']['median']:.2f} ms")
    print(f"   P95    : {metrics['total_time_ms']['p95']:.2f} ms")
    print(f"   P99    : {metrics['total_time_ms']['p99']:.2f} ms")
    print(f"   Min    : {metrics['total_time_ms']['min']:.2f} ms")
    print(f"   Max    : {metrics['total_time_ms']['max']:.2f} ms")
    print(f"   Std    : {metrics['total_time_ms']['std']:.2f} ms")

    print("\nğŸ”§ PREPROCESSING")
    print(f"   Mean   : {metrics['preprocessing_time_ms']['mean']:.2f} ms ({metrics['preprocessing_time_ms']['percentage_of_total']:.1f}%)")
    print(f"   Median : {metrics['preprocessing_time_ms']['median']:.2f} ms")
    print(f"   P95    : {metrics['preprocessing_time_ms']['p95']:.2f} ms")
    print(f"   P99    : {metrics['preprocessing_time_ms']['p99']:.2f} ms")

    print("\nğŸ¤– INFERENCE (LightGBM)")
    print(f"   Mean   : {metrics['inference_time_ms']['mean']:.2f} ms ({metrics['inference_time_ms']['percentage_of_total']:.1f}%)")
    print(f"   Median : {metrics['inference_time_ms']['median']:.2f} ms")
    print(f"   P95    : {metrics['inference_time_ms']['p95']:.2f} ms")
    print(f"   P99    : {metrics['inference_time_ms']['p99']:.2f} ms")

    print("\nğŸš€ THROUGHPUT")
    print(f"   {metrics['throughput']['predictions_per_second']:.2f} prÃ©dictions/seconde")
    print(f"   {metrics['throughput']['predictions_per_minute']:,} prÃ©dictions/minute")

    print("\nğŸ”´ GOULOT D'Ã‰TRANGLEMENT IDENTIFIÃ‰")
    print(f"   Preprocessing : {metrics['preprocessing_time_ms']['percentage_of_total']:.1f}% du temps total")
    print(f"   â†’ Focus optimisation sur ce composant")

    print("\n" + "="*70)


def main():
    """Fonction principale."""
    print("\nğŸš€ Calcul des mÃ©triques baseline depuis PostgreSQL\n")

    try:
        # Calculer les mÃ©triques
        metrics = calculate_metrics_from_postgres()

        # Afficher le rÃ©sumÃ©
        print_summary(metrics)

        # Sauvegarder le JSON
        output_path = Path("reports/benchmarks/baseline_production.json")
        save_metrics_to_json(metrics, output_path)

        print(f"\nâœ… TÃ¢che 1.1 terminÃ©e : MÃ©triques baseline PostgreSQL calculÃ©es")
        print(f"ğŸ“ Fichier gÃ©nÃ©rÃ© : {output_path}")

        return 0

    except Exception as e:
        print(f"\nâŒ Erreur : {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
