{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #1e3a8a; background-color: #dbeafe; padding: 12px; border-left: 5px solid #2563eb; margin: 20px 0; font-weight: bold;\">Modèle de Production - Home Credit Scoring API</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "**Objectif** : Entraîner le modèle LightGBM final optimisé et sauvegarder tous les artefacts nécessaires pour la production.\n",
    "\n",
    "**Artefacts générés** :\n",
    "- `model.pkl` : Modèle LightGBM entraîné\n",
    "- `label_encoders.pkl` : Dictionnaire des LabelEncoders\n",
    "- `onehot_encoder.pkl` : OneHotEncoder\n",
    "- `feature_names.pkl` : Liste des noms de features après encodage\n",
    "- `metrics.json` : Métriques de référence du modèle\n",
    "- `threshold.json` : Seuil de décision optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports terminés\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix, roc_curve\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Imports terminés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_data",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Chargement des Données</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHARGEMENT DES DONNÉES\n",
      "================================================================================\n",
      "\n",
      "Dataset chargé : app_train_models.csv\n",
      "Forme : (307511, 646)\n",
      "Colonnes : 646\n",
      "Observations : 307,511\n",
      "\n",
      "DISTRIBUTION DE LA TARGET:\n",
      "  Classe 0 (pas de défaut): 91.9%\n",
      "  Classe 1 (défaut): 8.1%\n",
      "  Ratio déséquilibre: 11.4:1\n",
      "\n",
      "Valeurs manquantes : 0\n",
      "\n",
      "Chargement terminé\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHARGEMENT DES DONNÉES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Chemin vers les données\n",
    "data_path = Path('../data/app_train_models.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Fichier non trouvé : {data_path}\")\n",
    "\n",
    "# Chargement\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset chargé : {data_path.name}\")\n",
    "print(f\"Forme : {df.shape}\")\n",
    "print(f\"Colonnes : {df.shape[1]}\")\n",
    "print(f\"Observations : {df.shape[0]:,}\")\n",
    "print()\n",
    "\n",
    "# Distribution de la target\n",
    "if 'TARGET' in df.columns:\n",
    "    target_dist = df['TARGET'].value_counts(normalize=True)\n",
    "    print(\"DISTRIBUTION DE LA TARGET:\")\n",
    "    print(f\"  Classe 0 (pas de défaut): {target_dist[0]:.1%}\")\n",
    "    print(f\"  Classe 1 (défaut): {target_dist[1]:.1%}\")\n",
    "    print(f\"  Ratio déséquilibre: {target_dist[0]/target_dist[1]:.1f}:1\")\n",
    "    print()\n",
    "\n",
    "# Vérification valeurs manquantes\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"Valeurs manquantes : {missing_count}\")\n",
    "print()\n",
    "\n",
    "print(\"Chargement terminé\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_split",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Split Train/Validation Stratifié</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "split_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLIT TRAIN/VALIDATION STRATIFIÉ\n",
      "================================================================================\n",
      "\n",
      "Features (X): (307511, 644)\n",
      "Target (y): (307511,)\n",
      "\n",
      "RÉSULTATS DU SPLIT:\n",
      "  Train set: 246,008 échantillons (80%)\n",
      "  Validation set: 61,503 échantillons (20%)\n",
      "\n",
      "VÉRIFICATION STRATIFICATION:\n",
      "Set          Classe 0   Classe 1   Ratio     \n",
      "--------------------------------------------------\n",
      "Original     91.9%      8.1%       11.4      \n",
      "Train        91.9%      8.1%       11.4      \n",
      "Validation   91.9%      8.1%       11.4      \n",
      "\n",
      "Split terminé\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SPLIT TRAIN/VALIDATION STRATIFIÉ\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Séparation features et target\n",
    "X = df.drop(['TARGET', 'SK_ID_CURR'], axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print()\n",
    "\n",
    "# Split stratifié 80/20\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"RÉSULTATS DU SPLIT:\")\n",
    "print(f\"  Train set: {X_train.shape[0]:,} échantillons ({(1-test_size)*100:.0f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]:,} échantillons ({test_size*100:.0f}%)\")\n",
    "print()\n",
    "\n",
    "# Vérification stratification\n",
    "original_proportions = y.value_counts(normalize=True)\n",
    "train_proportions = y_train.value_counts(normalize=True)\n",
    "val_proportions = y_val.value_counts(normalize=True)\n",
    "\n",
    "print(\"VÉRIFICATION STRATIFICATION:\")\n",
    "print(f\"{'Set':<12} {'Classe 0':<10} {'Classe 1':<10} {'Ratio':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original':<12} {original_proportions[0]:<10.1%} {original_proportions[1]:<10.1%} {original_proportions[0]/original_proportions[1]:<10.1f}\")\n",
    "print(f\"{'Train':<12} {train_proportions[0]:<10.1%} {train_proportions[1]:<10.1%} {train_proportions[0]/train_proportions[1]:<10.1f}\")\n",
    "print(f\"{'Validation':<12} {val_proportions[0]:<10.1%} {val_proportions[1]:<10.1%} {val_proportions[0]/val_proportions[1]:<10.1f}\")\n",
    "print()\n",
    "\n",
    "print(\"Split terminé\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_encoding",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Encodage des Variables Catégorielles</h2>\n",
    "\n",
    "**Stratégie** :\n",
    "- **≤2 catégories** → Label Encoding (0/1)\n",
    "- **>2 catégories** → One-Hot Encoding\n",
    "- **Fit sur train uniquement** → Transform sur train et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "encoding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENCODAGE DES VARIABLES CATÉGORIELLES\n",
      "================================================================================\n",
      "\n",
      "Variables catégorielles détectées: 37\n",
      "\n",
      "ANALYSE DES VARIABLES CATÉGORIELLES:\n",
      "  • NAME_CONTRACT_TYPE: 2 catégories → Label Encoding\n",
      "  • CODE_GENDER: 3 catégories → One-Hot Encoding\n",
      "  • FLAG_OWN_CAR: 2 catégories → Label Encoding\n",
      "  • FLAG_OWN_REALTY: 2 catégories → Label Encoding\n",
      "  • NAME_TYPE_SUITE: 7 catégories → One-Hot Encoding\n",
      "  • NAME_INCOME_TYPE: 8 catégories → One-Hot Encoding\n",
      "  • NAME_EDUCATION_TYPE: 5 catégories → One-Hot Encoding\n",
      "  • NAME_FAMILY_STATUS: 6 catégories → One-Hot Encoding\n",
      "  • NAME_HOUSING_TYPE: 6 catégories → One-Hot Encoding\n",
      "  • OCCUPATION_TYPE: 18 catégories → One-Hot Encoding\n",
      "  • WEEKDAY_APPR_PROCESS_START: 7 catégories → One-Hot Encoding\n",
      "  • ORGANIZATION_TYPE: 58 catégories → One-Hot Encoding\n",
      "  • FONDKAPREMONT_MODE: 4 catégories → One-Hot Encoding\n",
      "  • HOUSETYPE_MODE: 3 catégories → One-Hot Encoding\n",
      "  • WALLSMATERIAL_MODE: 7 catégories → One-Hot Encoding\n",
      "  • EMERGENCYSTATE_MODE: 2 catégories → Label Encoding\n",
      "  • BUREAU_CREDIT_ACTIVE_<LAMBDA_0>: 3 catégories → One-Hot Encoding\n",
      "  • BUREAU_CREDIT_CURRENCY_<LAMBDA_0>: 3 catégories → One-Hot Encoding\n",
      "  • BUREAU_CREDIT_TYPE_<LAMBDA_0>: 11 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_CONTRACT_TYPE_<LAMBDA_0>: 4 catégories → One-Hot Encoding\n",
      "  • PREV_WEEKDAY_APPR_PROCESS_START_<LAMBDA_0>: 7 catégories → One-Hot Encoding\n",
      "  • PREV_FLAG_LAST_APPL_PER_CONTRACT_<LAMBDA_0>: 2 catégories → Label Encoding\n",
      "  • PREV_NAME_CASH_LOAN_PURPOSE_<LAMBDA_0>: 25 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_CONTRACT_STATUS_<LAMBDA_0>: 4 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_PAYMENT_TYPE_<LAMBDA_0>: 4 catégories → One-Hot Encoding\n",
      "  • PREV_CODE_REJECT_REASON_<LAMBDA_0>: 9 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_TYPE_SUITE_<LAMBDA_0>: 7 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_CLIENT_TYPE_<LAMBDA_0>: 4 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_GOODS_CATEGORY_<LAMBDA_0>: 26 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_PORTFOLIO_<LAMBDA_0>: 5 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_PRODUCT_TYPE_<LAMBDA_0>: 3 catégories → One-Hot Encoding\n",
      "  • PREV_CHANNEL_TYPE_<LAMBDA_0>: 8 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_SELLER_INDUSTRY_<LAMBDA_0>: 11 catégories → One-Hot Encoding\n",
      "  • PREV_NAME_YIELD_GROUP_<LAMBDA_0>: 5 catégories → One-Hot Encoding\n",
      "  • PREV_PRODUCT_COMBINATION_<LAMBDA_0>: 17 catégories → One-Hot Encoding\n",
      "  • POS_NAME_CONTRACT_STATUS_<LAMBDA_0>: 7 catégories → One-Hot Encoding\n",
      "  • CC_NAME_CONTRACT_STATUS_<LAMBDA_0>: 4 catégories → One-Hot Encoding\n",
      "\n",
      "Label Encoding (≤2 cat): 5\n",
      "One-Hot Encoding (>2 cat): 32\n",
      "\n",
      "APPLICATION LABEL ENCODING:\n",
      "  Encodage: NAME_CONTRACT_TYPE\n",
      "  Encodage: FLAG_OWN_CAR\n",
      "  Encodage: FLAG_OWN_REALTY\n",
      "  Encodage: EMERGENCYSTATE_MODE\n",
      "  Encodage: PREV_FLAG_LAST_APPL_PER_CONTRACT_<LAMBDA_0>\n",
      "\n",
      "APPLICATION ONE-HOT ENCODING:\n",
      "  Encodage: CODE_GENDER\n",
      "  Encodage: NAME_TYPE_SUITE\n",
      "  Encodage: NAME_INCOME_TYPE\n",
      "  Encodage: NAME_EDUCATION_TYPE\n",
      "  Encodage: NAME_FAMILY_STATUS\n",
      "  Encodage: NAME_HOUSING_TYPE\n",
      "  Encodage: OCCUPATION_TYPE\n",
      "  Encodage: WEEKDAY_APPR_PROCESS_START\n",
      "  Encodage: ORGANIZATION_TYPE\n",
      "  Encodage: FONDKAPREMONT_MODE\n",
      "  Encodage: HOUSETYPE_MODE\n",
      "  Encodage: WALLSMATERIAL_MODE\n",
      "  Encodage: BUREAU_CREDIT_ACTIVE_<LAMBDA_0>\n",
      "  Encodage: BUREAU_CREDIT_CURRENCY_<LAMBDA_0>\n",
      "  Encodage: BUREAU_CREDIT_TYPE_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_CONTRACT_TYPE_<LAMBDA_0>\n",
      "  Encodage: PREV_WEEKDAY_APPR_PROCESS_START_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_CASH_LOAN_PURPOSE_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_CONTRACT_STATUS_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_PAYMENT_TYPE_<LAMBDA_0>\n",
      "  Encodage: PREV_CODE_REJECT_REASON_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_TYPE_SUITE_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_CLIENT_TYPE_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_GOODS_CATEGORY_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_PORTFOLIO_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_PRODUCT_TYPE_<LAMBDA_0>\n",
      "  Encodage: PREV_CHANNEL_TYPE_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_SELLER_INDUSTRY_<LAMBDA_0>\n",
      "  Encodage: PREV_NAME_YIELD_GROUP_<LAMBDA_0>\n",
      "  Encodage: PREV_PRODUCT_COMBINATION_<LAMBDA_0>\n",
      "  Encodage: POS_NAME_CONTRACT_STATUS_<LAMBDA_0>\n",
      "  Encodage: CC_NAME_CONTRACT_STATUS_<LAMBDA_0>\n",
      "\n",
      "ENCODAGE TERMINÉ:\n",
      "  • Label Encoders créés: 5\n",
      "  • One-Hot Encoders créés: 32\n",
      "\n",
      "VÉRIFICATION POST-ENCODAGE:\n",
      "  Variables 'object' restantes:\n",
      "    Train: 0\n",
      "    Validation: 0\n",
      "  ENCODAGE COMPLET - Tous datasets 100% numériques\n",
      "\n",
      "Datasets finaux après encodage:\n",
      "  Train: (246008, 911)\n",
      "  Validation: (61503, 911)\n",
      "\n",
      "Nettoyage des noms de colonnes pour LightGBM...\n",
      "Nettoyage terminé\n",
      "\n",
      "Noms de features sauvegardés : 911 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ENCODAGE DES VARIABLES CATÉGORIELLES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Identifier les variables catégorielles\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Variables catégorielles détectées: {len(categorical_cols)}\")\n",
    "print()\n",
    "\n",
    "if len(categorical_cols) == 0:\n",
    "    print(\"Aucune variable catégorielle - Datasets déjà numériques\")\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "    label_encoders = {}\n",
    "    onehot_encoders = {}\n",
    "else:\n",
    "    # Analyser les variables catégorielles\n",
    "    print(\"ANALYSE DES VARIABLES CATÉGORIELLES:\")\n",
    "    cols_label_encoding = []\n",
    "    cols_onehot_encoding = []\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        n_categories = X_train[col].nunique()\n",
    "        if n_categories <= 2:\n",
    "            cols_label_encoding.append(col)\n",
    "            strategy = \"Label Encoding\"\n",
    "        else:\n",
    "            cols_onehot_encoding.append(col)\n",
    "            strategy = \"One-Hot Encoding\"\n",
    "        print(f\"  • {col}: {n_categories} catégories → {strategy}\")\n",
    "\n",
    "    print()\n",
    "    print(f\"Label Encoding (≤2 cat): {len(cols_label_encoding)}\")\n",
    "    print(f\"One-Hot Encoding (>2 cat): {len(cols_onehot_encoding)}\")\n",
    "    print()\n",
    "\n",
    "    # Copier les datasets\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "\n",
    "    # Dictionnaires pour les encodeurs\n",
    "    label_encoders = {}\n",
    "    onehot_encoders = {}\n",
    "\n",
    "    # LABEL ENCODING\n",
    "    if len(cols_label_encoding) > 0:\n",
    "        print(\"APPLICATION LABEL ENCODING:\")\n",
    "        for col in cols_label_encoding:\n",
    "            print(f\"  Encodage: {col}\")\n",
    "\n",
    "            # Fit sur train uniquement\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X_train_encoded[col])\n",
    "\n",
    "            # Transform train\n",
    "            X_train_encoded[col] = le.transform(X_train_encoded[col])\n",
    "\n",
    "            # Transform validation (gestion catégories nouvelles)\n",
    "            val_unknown = ~X_val_encoded[col].isin(le.classes_)\n",
    "            if val_unknown.sum() > 0:\n",
    "                print(f\"    {val_unknown.sum()} catégories nouvelles en validation\")\n",
    "                most_frequent = X_train[col].mode()[0]\n",
    "                X_val_encoded.loc[val_unknown, col] = most_frequent\n",
    "\n",
    "            X_val_encoded[col] = le.transform(X_val_encoded[col])\n",
    "            label_encoders[col] = le\n",
    "        print()\n",
    "    # ONE-HOT ENCODING\n",
    "    if len(cols_onehot_encoding) > 0:\n",
    "        print(\"APPLICATION ONE-HOT ENCODING:\")\n",
    "        for col in cols_onehot_encoding:\n",
    "            print(f\"  Encodage: {col}\")\n",
    "\n",
    "            # Fit sur train uniquement\n",
    "            ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            ohe.fit(X_train_encoded[[col]])\n",
    "\n",
    "            # Noms des nouvelles colonnes\n",
    "            feature_names_temp = [f\"{col}_{cat}\" for cat in ohe.categories_[0]]\n",
    "\n",
    "            # Transform train\n",
    "            train_encoded = ohe.transform(X_train_encoded[[col]])\n",
    "            train_ohe_df = pd.DataFrame(train_encoded, columns=feature_names_temp, index=X_train_encoded.index)\n",
    "            X_train_encoded = X_train_encoded.drop(columns=[col])\n",
    "            X_train_encoded = pd.concat([X_train_encoded, train_ohe_df], axis=1)\n",
    "\n",
    "            # Transform validation\n",
    "            val_encoded = ohe.transform(X_val_encoded[[col]])\n",
    "            val_ohe_df = pd.DataFrame(val_encoded, columns=feature_names_temp, index=X_val_encoded.index)\n",
    "            X_val_encoded = X_val_encoded.drop(columns=[col])\n",
    "            X_val_encoded = pd.concat([X_val_encoded, val_ohe_df], axis=1)\n",
    "\n",
    "            onehot_encoders[col] = ohe\n",
    "        print()\n",
    "\n",
    "    print(\"ENCODAGE TERMINÉ:\")\n",
    "    print(f\"  • Label Encoders créés: {len(label_encoders)}\")\n",
    "    print(f\"  • One-Hot Encoders créés: {len(onehot_encoders)}\")\n",
    "    print()\n",
    "\n",
    "# Vérification finale\n",
    "remaining_objects_train = X_train_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "remaining_objects_val = X_val_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"VÉRIFICATION POST-ENCODAGE:\")\n",
    "print(f\"  Variables 'object' restantes:\")\n",
    "print(f\"    Train: {len(remaining_objects_train)}\")\n",
    "print(f\"    Validation: {len(remaining_objects_val)}\")\n",
    "\n",
    "if len(remaining_objects_train) == 0 and len(remaining_objects_val) == 0:\n",
    "    print(\"  ENCODAGE COMPLET - Tous datasets 100% numériques\")\n",
    "else:\n",
    "    print(f\"  ATTENTION - Variables non encodées détectées\")\n",
    "\n",
    "print()\n",
    "print(\"Datasets finaux après encodage:\")\n",
    "print(f\"  Train: {X_train_encoded.shape}\")\n",
    "print(f\"  Validation: {X_val_encoded.shape}\")\n",
    "print()\n",
    "\n",
    "# Nettoyer les noms de colonnes pour LightGBM\n",
    "print(\"Nettoyage des noms de colonnes pour LightGBM...\")\n",
    "X_train_encoded.columns = X_train_encoded.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "X_val_encoded.columns = X_val_encoded.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "print(\"Nettoyage terminé\")\n",
    "print()\n",
    "\n",
    "# Sauvegarder les noms de features\n",
    "feature_names = X_train_encoded.columns.tolist()\n",
    "print(f\"Noms de features sauvegardés : {len(feature_names)} features\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_model",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Entraînement du Modèle LightGBM Optimisé</h2>\n",
    "\n",
    "**Hyperparamètres optimaux** (trouvés via Optuna - Trial 41) :\n",
    "- `n_estimators` : 300\n",
    "- `max_depth` : 7\n",
    "- `learning_rate` : 0.0583\n",
    "- `num_leaves` : 40\n",
    "- `min_child_samples` : 30\n",
    "- `subsample` : 0.7\n",
    "- `colsample_bytree` : 0.7\n",
    "- `reg_alpha` : 0.9\n",
    "- `reg_lambda` : 0.8\n",
    "\n",
    "**Coût métier baseline** : 30,118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTRAÎNEMENT DU MODÈLE LIGHTGBM OPTIMISÉ\n",
      "================================================================================\n",
      "\n",
      "HYPERPARAMÈTRES:\n",
      "  • n_estimators         : 300\n",
      "  • max_depth            : 7\n",
      "  • learning_rate        : 0.058264831213179456\n",
      "  • num_leaves           : 40\n",
      "  • min_child_samples    : 30\n",
      "  • subsample            : 0.7\n",
      "  • colsample_bytree     : 0.7\n",
      "  • reg_alpha            : 0.9\n",
      "  • reg_lambda           : 0.8\n",
      "  • class_weight         : balanced\n",
      "  • random_state         : 42\n",
      "\n",
      "Création du modèle LightGBM...\n",
      "Modèle créé\n",
      "\n",
      "Entraînement sur 246,008 échantillons...\n",
      "Entraînement terminé en 18.26s\n",
      "\n",
      "Modèle entraîné avec succès\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ENTRAÎNEMENT DU MODÈLE LIGHTGBM OPTIMISÉ\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Hyperparamètres optimaux (du meilleur trial Optuna)\n",
    "best_params = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.058264831213179456,\n",
    "    'num_leaves': 40,\n",
    "    'min_child_samples': 30,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.9,\n",
    "    'reg_lambda': 0.8,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"HYPERPARAMÈTRES:\")\n",
    "for param, value in best_params.items():\n",
    "    if param != 'verbose':\n",
    "        print(f\"  • {param:<20} : {value}\")\n",
    "print()\n",
    "\n",
    "# Créer le modèle\n",
    "print(\"Création du modèle LightGBM...\")\n",
    "model = LGBMClassifier(**best_params)\n",
    "print(\"Modèle créé\")\n",
    "print()\n",
    "\n",
    "# Entraînement\n",
    "print(f\"Entraînement sur {X_train_encoded.shape[0]:,} échantillons...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Entraînement terminé en {train_time:.2f}s\")\n",
    "print()\n",
    "\n",
    "print(\"Modèle entraîné avec succès\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_eval",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Évaluation sur Validation Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "evaluate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ÉVALUATION SUR VALIDATION SET\n",
      "================================================================================\n",
      "\n",
      "Génération des prédictions...\n",
      "Prédictions générées\n",
      "\n",
      "MÉTRIQUES VALIDATION (Seuil 0.5):\n",
      "  • AUC-ROC   : 0.7826\n",
      "  • Accuracy  : 0.7492\n",
      "  • Precision : 0.1931\n",
      "  • Recall    : 0.6628\n",
      "  • F1-Score  : 0.2991\n",
      "\n",
      "MATRICE DE CONFUSION:\n",
      "  • TN (Vrai Négatif)  : 42,788\n",
      "  • FP (Faux Positif)  : 13,750\n",
      "  • FN (Faux Négatif)  : 1,674\n",
      "  • TP (Vrai Positif)  : 3,291\n",
      "\n",
      "COÛT MÉTIER (Seuil 0.5):\n",
      "  • Coût FN (défauts manqués)      : 16,740 (1,674 x 10)\n",
      "  • Coût FP (bons clients refusés) : 13,750 (13,750 x 1)\n",
      "  • COÛT TOTAL                     : 30,490\n",
      "\n",
      "Évaluation terminée\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ÉVALUATION SUR VALIDATION SET\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Prédictions\n",
    "print(\"Génération des prédictions...\")\n",
    "y_val_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "y_val_pred = model.predict(X_val_encoded)\n",
    "print(\"Prédictions générées\")\n",
    "print()\n",
    "\n",
    "# Métriques (seuil par défaut 0.5)\n",
    "print(\"MÉTRIQUES VALIDATION (Seuil 0.5):\")\n",
    "auc_roc = roc_auc_score(y_val, y_val_proba)\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "recall = recall_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"  • AUC-ROC   : {auc_roc:.4f}\")\n",
    "print(f\"  • Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"  • Precision : {precision:.4f}\")\n",
    "print(f\"  • Recall    : {recall:.4f}\")\n",
    "print(f\"  • F1-Score  : {f1:.4f}\")\n",
    "print()\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"MATRICE DE CONFUSION:\")\n",
    "print(f\"  • TN (Vrai Négatif)  : {tn:,}\")\n",
    "print(f\"  • FP (Faux Positif)  : {fp:,}\")\n",
    "print(f\"  • FN (Faux Négatif)  : {fn:,}\")\n",
    "print(f\"  • TP (Vrai Positif)  : {tp:,}\")\n",
    "print()\n",
    "\n",
    "# Coût métier (seuil 0.5)\n",
    "cost_fn = fn * 10  # Faux négatifs coûtent 10x\n",
    "cost_fp = fp * 1   # Faux positifs coûtent 1x\n",
    "total_cost = cost_fn + cost_fp\n",
    "\n",
    "print(\"COÛT MÉTIER (Seuil 0.5):\")\n",
    "print(f\"  • Coût FN (défauts manqués)      : {cost_fn:,} ({fn:,} x 10)\")\n",
    "print(f\"  • Coût FP (bons clients refusés) : {cost_fp:,} ({fp:,} x 1)\")\n",
    "print(f\"  • COÛT TOTAL                     : {total_cost:,}\")\n",
    "print()\n",
    "\n",
    "print(\"Évaluation terminée\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_threshold",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Optimisation du Seuil de Décision</h2>\n",
    "\n",
    "**Objectif** : Trouver le seuil qui minimise le coût métier total\n",
    "\n",
    "**Coût métier** : `Coût = (10 × FN) + (1 × FP)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "optimize_threshold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTIMISATION DU SEUIL DE DÉCISION\n",
      "================================================================================\n",
      "\n",
      "Test de 90 seuils...\n",
      "Seuil optimal trouvé : 0.5225\n",
      "\n",
      "RÉSULTATS AU SEUIL OPTIMAL:\n",
      "  • Seuil optimal          : 0.5225\n",
      "  • Coût optimal           : 30,437\n",
      "  • Coût au seuil 0.5      : 30,490\n",
      "  • Économie réalisée      : 53 (0.2%)\n",
      "\n",
      "MÉTRIQUES AU SEUIL OPTIMAL:\n",
      "  • Recall    : 0.6389\n",
      "  • Precision : 0.2023\n",
      "  • F1-Score  : 0.3073\n",
      "\n",
      "MATRICE DE CONFUSION (Seuil optimal):\n",
      "  • TN (Vrai Négatif)  : 44,031\n",
      "  • FP (Faux Positif)  : 12,507\n",
      "  • FN (Faux Négatif)  : 1,793\n",
      "  • TP (Vrai Positif)  : 3,172\n",
      "\n",
      "Optimisation du seuil terminée\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"OPTIMISATION DU SEUIL DE DÉCISION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Tester différents seuils\n",
    "thresholds_to_test = np.linspace(0.1, 0.9, 90)\n",
    "costs = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "print(f\"Test de {len(thresholds_to_test)} seuils...\")\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    # Prédictions avec ce seuil\n",
    "    y_pred_threshold = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm_t = confusion_matrix(y_val, y_pred_threshold)\n",
    "    tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
    "\n",
    "    # Coût métier\n",
    "    cost = (fn_t * 10) + (fp_t * 1)\n",
    "    costs.append(cost)\n",
    "\n",
    "    # Métriques\n",
    "    recall_t = recall_score(y_val, y_pred_threshold)\n",
    "    precision_t = precision_score(y_val, y_pred_threshold)\n",
    "    recalls.append(recall_t)\n",
    "    precisions.append(precision_t)\n",
    "\n",
    "# Trouver le seuil optimal\n",
    "optimal_idx = np.argmin(costs)\n",
    "optimal_threshold = thresholds_to_test[optimal_idx]\n",
    "optimal_cost = costs[optimal_idx]\n",
    "optimal_recall = recalls[optimal_idx]\n",
    "optimal_precision = precisions[optimal_idx]\n",
    "\n",
    "print(f\"Seuil optimal trouvé : {optimal_threshold:.4f}\")\n",
    "print()\n",
    "\n",
    "# Métriques au seuil optimal\n",
    "y_val_pred_optimal = (y_val_proba >= optimal_threshold).astype(int)\n",
    "cm_optimal = confusion_matrix(y_val, y_val_pred_optimal)\n",
    "tn_opt, fp_opt, fn_opt, tp_opt = cm_optimal.ravel()\n",
    "\n",
    "print(\"RÉSULTATS AU SEUIL OPTIMAL:\")\n",
    "print(f\"  • Seuil optimal          : {optimal_threshold:.4f}\")\n",
    "print(f\"  • Coût optimal           : {optimal_cost:,}\")\n",
    "print(f\"  • Coût au seuil 0.5      : {total_cost:,}\")\n",
    "print(f\"  • Économie réalisée      : {total_cost - optimal_cost:,} ({((total_cost - optimal_cost)/total_cost)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"MÉTRIQUES AU SEUIL OPTIMAL:\")\n",
    "print(f\"  • Recall    : {optimal_recall:.4f}\")\n",
    "print(f\"  • Precision : {optimal_precision:.4f}\")\n",
    "print(f\"  • F1-Score  : {2 * (optimal_precision * optimal_recall) / (optimal_precision + optimal_recall):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"MATRICE DE CONFUSION (Seuil optimal):\")\n",
    "print(f\"  • TN (Vrai Négatif)  : {tn_opt:,}\")\n",
    "print(f\"  • FP (Faux Positif)  : {fp_opt:,}\")\n",
    "print(f\"  • FN (Faux Négatif)  : {fn_opt:,}\")\n",
    "print(f\"  • TP (Vrai Positif)  : {tp_opt:,}\")\n",
    "print()\n",
    "\n",
    "print(\"Optimisation du seuil terminée\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_save",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Sauvegarde des Artefacts de Production</h2>\n",
    "\n",
    "**Artefacts sauvegardés dans `../models/`** :\n",
    "1. `model.pkl` - Modèle LightGBM entraîné\n",
    "2. `label_encoders.pkl` - Dictionnaire des LabelEncoders\n",
    "3. `onehot_encoder.pkl` - OneHotEncoder (dict)\n",
    "4. `feature_names.pkl` - Liste des noms de features\n",
    "5. `metrics.json` - Métriques de référence\n",
    "6. `threshold.json` - Seuil de décision optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAUVEGARDE DES ARTEFACTS DE PRODUCTION\n",
      "================================================================================\n",
      "\n",
      "Dossier de destination : /Users/mounirmeknaci/Desktop/Data_Projects/Projet8/models\n",
      "\n",
      "1. Sauvegarde du modèle LightGBM...\n",
      "   ✓ Modèle sauvegardé : model.pkl\n",
      "\n",
      "2. Sauvegarde des Label Encoders...\n",
      "   ✓ Label Encoders sauvegardés : label_encoders.pkl\n",
      "   • Nombre d'encodeurs : 5\n",
      "\n",
      "3. Sauvegarde des One-Hot Encoders...\n",
      "   ✓ One-Hot Encoders sauvegardés : onehot_encoder.pkl\n",
      "   • Nombre d'encodeurs : 32\n",
      "\n",
      "4. Sauvegarde des noms de features...\n",
      "   ✓ Noms de features sauvegardés : feature_names.pkl\n",
      "   • Nombre de features : 911\n",
      "\n",
      "5. Sauvegarde des métriques de référence...\n",
      "   ✓ Métriques sauvegardées : metrics.json\n",
      "\n",
      "6. Sauvegarde du seuil optimal...\n",
      "   ✓ Seuil optimal sauvegardé : threshold.json\n",
      "\n",
      "================================================================================\n",
      "TOUS LES ARTEFACTS ONT ÉTÉ SAUVEGARDÉS AVEC SUCCÈS\n",
      "================================================================================\n",
      "\n",
      "FICHIERS CRÉÉS:\n",
      "  • .gitkeep                  : 0 B\n",
      "  • feature_names.pkl         : 29.2 KB\n",
      "  • label_encoders.pkl        : 615 B\n",
      "  • metrics.json              : 824 B\n",
      "  • model.pkl                 : 1.4 MB\n",
      "  • onehot_encoder.pkl        : 10.0 KB\n",
      "  • threshold.json            : 195 B\n",
      "\n",
      "Le modèle est prêt pour la production !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAUVEGARDE DES ARTEFACTS DE PRODUCTION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Créer le dossier models s'il n'existe pas\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "print(f\"Dossier de destination : {models_dir.resolve()}\")\n",
    "print()\n",
    "\n",
    "# 1. Sauvegarder le modèle\n",
    "print(\"1. Sauvegarde du modèle LightGBM...\")\n",
    "model_path = models_dir / 'model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"   ✓ Modèle sauvegardé : {model_path.name}\")\n",
    "print()\n",
    "\n",
    "# 2. Sauvegarder les label encoders\n",
    "print(\"2. Sauvegarde des Label Encoders...\")\n",
    "label_encoders_path = models_dir / 'label_encoders.pkl'\n",
    "with open(label_encoders_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(f\"   ✓ Label Encoders sauvegardés : {label_encoders_path.name}\")\n",
    "print(f\"   • Nombre d'encodeurs : {len(label_encoders)}\")\n",
    "print()\n",
    "\n",
    "# 3. Sauvegarder les one-hot encoders\n",
    "print(\"3. Sauvegarde des One-Hot Encoders...\")\n",
    "onehot_encoders_path = models_dir / 'onehot_encoder.pkl'\n",
    "with open(onehot_encoders_path, 'wb') as f:\n",
    "    pickle.dump(onehot_encoders, f)\n",
    "print(f\"   ✓ One-Hot Encoders sauvegardés : {onehot_encoders_path.name}\")\n",
    "print(f\"   • Nombre d'encodeurs : {len(onehot_encoders)}\")\n",
    "print()\n",
    "\n",
    "# 4. Sauvegarder les noms de features\n",
    "print(\"4. Sauvegarde des noms de features...\")\n",
    "feature_names_path = models_dir / 'feature_names.pkl'\n",
    "with open(feature_names_path, 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"   ✓ Noms de features sauvegardés : {feature_names_path.name}\")\n",
    "print(f\"   • Nombre de features : {len(feature_names)}\")\n",
    "print()\n",
    "\n",
    "# 5. Sauvegarder les métriques de référence\n",
    "print(\"5. Sauvegarde des métriques de référence...\")\n",
    "metrics = {\n",
    "    'auc_roc': float(auc_roc),\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'threshold_default': 0.5,\n",
    "    'confusion_matrix': {\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'tp': int(tp)\n",
    "    },\n",
    "    'business_cost': {\n",
    "        'cost_fn': int(cost_fn),\n",
    "        'cost_fp': int(cost_fp),\n",
    "        'total_cost': int(total_cost)\n",
    "    },\n",
    "    'optimal_threshold': {\n",
    "        'threshold': float(optimal_threshold),\n",
    "        'recall': float(optimal_recall),\n",
    "        'precision': float(optimal_precision),\n",
    "        'total_cost': int(optimal_cost),\n",
    "        'savings': int(total_cost - optimal_cost),\n",
    "        'confusion_matrix': {\n",
    "            'tn': int(tn_opt),\n",
    "            'fp': int(fp_opt),\n",
    "            'fn': int(fn_opt),\n",
    "            'tp': int(tp_opt)\n",
    "        }\n",
    "    },\n",
    "    'training_info': {\n",
    "        'train_samples': int(X_train_encoded.shape[0]),\n",
    "        'val_samples': int(X_val_encoded.shape[0]),\n",
    "        'n_features': int(len(feature_names)),\n",
    "        'train_time_seconds': float(train_time)\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = models_dir / 'metrics.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"   ✓ Métriques sauvegardées : {metrics_path.name}\")\n",
    "print()\n",
    "\n",
    "# 6. Sauvegarder le seuil optimal\n",
    "print(\"6. Sauvegarde du seuil optimal...\")\n",
    "threshold_data = {\n",
    "    'optimal_threshold': float(optimal_threshold),\n",
    "    'default_threshold': 0.5,\n",
    "    'description': 'Seuil de décision optimisé pour minimiser le coût métier (10×FN + 1×FP)'\n",
    "}\n",
    "\n",
    "threshold_path = models_dir / 'threshold.json'\n",
    "with open(threshold_path, 'w') as f:\n",
    "    json.dump(threshold_data, f, indent=2)\n",
    "print(f\"   ✓ Seuil optimal sauvegardé : {threshold_path.name}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOUS LES ARTEFACTS ONT ÉTÉ SAUVEGARDÉS AVEC SUCCÈS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Résumé des fichiers créés\n",
    "print(\"FICHIERS CRÉÉS:\")\n",
    "for file in sorted(models_dir.iterdir()):\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size\n",
    "        if size < 1024:\n",
    "            size_str = f\"{size} B\"\n",
    "        elif size < 1024*1024:\n",
    "            size_str = f\"{size/1024:.1f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{size/(1024*1024):.1f} MB\"\n",
    "        print(f\"  • {file.name:<25} : {size_str}\")\n",
    "print()\n",
    "\n",
    "print(\"Le modèle est prêt pour la production !\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_summary",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1e40af; background-color: #eff6ff; padding: 10px; border-left: 4px solid #3b82f6; margin: 15px 0;\">Résumé Final</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RÉSUMÉ FINAL - MODÈLE DE PRODUCTION\n",
      "================================================================================\n",
      "\n",
      "MODÈLE:\n",
      "  • Algorithme           : LightGBM\n",
      "  • N° estimators        : 300\n",
      "  • Max depth            : 7\n",
      "  • Learning rate        : 0.0583\n",
      "\n",
      "DONNÉES:\n",
      "  • Dataset source       : app_train_models.csv\n",
      "  • Observations totales : 307,511\n",
      "  • Features finales     : 911\n",
      "  • Train samples        : 246,008\n",
      "  • Validation samples   : 61,503\n",
      "\n",
      "PERFORMANCES (Validation Set):\n",
      "  • AUC-ROC              : 0.7826\n",
      "  • Recall               : 0.6628\n",
      "  • Precision            : 0.1931\n",
      "  • F1-Score             : 0.2991\n",
      "\n",
      "COÛT MÉTIER:\n",
      "  • Seuil par défaut (0.5)\n",
      "    - Coût total         : 30,490\n",
      "  • Seuil optimal (0.5225)\n",
      "    - Coût total         : 30,437\n",
      "    - Économie           : 53 (0.2%)\n",
      "\n",
      "ARTEFACTS SAUVEGARDÉS:\n",
      "  • Emplacement          : /Users/mounirmeknaci/Desktop/Data_Projects/Projet8/models\n",
      "  • Nombre de fichiers   : 6\n",
      "    - model.pkl\n",
      "    - label_encoders.pkl\n",
      "    - onehot_encoder.pkl\n",
      "    - feature_names.pkl\n",
      "    - metrics.json\n",
      "    - threshold.json\n",
      "\n",
      "================================================================================\n",
      "NOTEBOOK TERMINÉ - MODÈLE PRÊT POUR LA PRODUCTION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RÉSUMÉ FINAL - MODÈLE DE PRODUCTION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"MODÈLE:\")\n",
    "print(f\"  • Algorithme           : LightGBM\")\n",
    "print(f\"  • N° estimators        : {best_params['n_estimators']}\")\n",
    "print(f\"  • Max depth            : {best_params['max_depth']}\")\n",
    "print(f\"  • Learning rate        : {best_params['learning_rate']:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"DONNÉES:\")\n",
    "print(f\"  • Dataset source       : app_train_models.csv\")\n",
    "print(f\"  • Observations totales : {df.shape[0]:,}\")\n",
    "print(f\"  • Features finales     : {len(feature_names)}\")\n",
    "print(f\"  • Train samples        : {X_train_encoded.shape[0]:,}\")\n",
    "print(f\"  • Validation samples   : {X_val_encoded.shape[0]:,}\")\n",
    "print()\n",
    "\n",
    "print(\"PERFORMANCES (Validation Set):\")\n",
    "print(f\"  • AUC-ROC              : {auc_roc:.4f}\")\n",
    "print(f\"  • Recall               : {recall:.4f}\")\n",
    "print(f\"  • Precision            : {precision:.4f}\")\n",
    "print(f\"  • F1-Score             : {f1:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"COÛT MÉTIER:\")\n",
    "print(f\"  • Seuil par défaut (0.5)\")\n",
    "print(f\"    - Coût total         : {total_cost:,}\")\n",
    "print(f\"  • Seuil optimal ({optimal_threshold:.4f})\")\n",
    "print(f\"    - Coût total         : {optimal_cost:,}\")\n",
    "print(f\"    - Économie           : {total_cost - optimal_cost:,} ({((total_cost - optimal_cost)/total_cost)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"ARTEFACTS SAUVEGARDÉS:\")\n",
    "print(f\"  • Emplacement          : {models_dir.resolve()}\")\n",
    "print(f\"  • Nombre de fichiers   : 6\")\n",
    "print(f\"    - model.pkl\")\n",
    "print(f\"    - label_encoders.pkl\")\n",
    "print(f\"    - onehot_encoder.pkl\")\n",
    "print(f\"    - feature_names.pkl\")\n",
    "print(f\"    - metrics.json\")\n",
    "print(f\"    - threshold.json\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK TERMINÉ - MODÈLE PRÊT POUR LA PRODUCTION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2a111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projet6)",
   "language": "python",
   "name": "projet6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
